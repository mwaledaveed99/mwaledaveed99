[
  {
    "objectID": "stat_modelling.html",
    "href": "stat_modelling.html",
    "title": "stat_modelling",
    "section": "",
    "text": "In this project we will be using a subset of the data from the Behavioral Risk Factor Surveillance System (BRFSS) from the CDC to investigate possible risk factors for arthritis. The BRFSS is a system of health-related telephone surveys that collect state data about U.S. residents regarding their health-related risk behaviors, chronic health conditions, and use of preventive services. It was established in 1984 with 15 states, BRFSS now collects data in all 50 states. This dataset contains health and social information about non-institutionalized adults in the US in 2013. There are 359,925 individuals included (rows) and 16 variables (columns). The goal is of the study is to investigate risk factors associated with arthritis."
  },
  {
    "objectID": "stat_modelling.html#removing-outliers",
    "href": "stat_modelling.html#removing-outliers",
    "title": "stat_modelling",
    "section": "Removing outliers",
    "text": "Removing outliers\n\nQ &lt;- quantile(lean_brfss$bmi, probs=c(.25, .75), na.rm = FALSE) # Specifying the lower and upper quatiles\niqr &lt;- IQR(lean_brfss$bmi) # Interquatile range\nup &lt;-  Q[2]+1.5*iqr # Upper Range  \nlow&lt;- Q[1]-1.5*iqr # Lower Range\nbrfss_eliminated&lt;- subset(lean_brfss, lean_brfss$bmi &gt; (Q[1] - 1.5*iqr) & lean_brfss$bmi &lt; (Q[2]+1.5*iqr))\n\n\nwith_outliers&lt;-lean_brfss |&gt; \n  ggplot(aes(x = bmi)) +\n  geom_boxplot(aes(),color = \"steelblue\") +\n  coord_flip() +\n  #facet_grid(~arthritis)+\n  theme_bw() +\n  labs(title = \"With outliers\")\n\n\nwithout_outliers&lt;-brfss_eliminated |&gt; \n  ggplot(aes(x = bmi)) +\n  geom_boxplot(aes(),color = \"steelblue\") +\n  coord_flip() +\n  #facet_grid(~arthritis)+\n  theme_bw() +\n  labs(title = \"Without outliers\")\n\n\ncowplot::plot_grid(with_outliers, without_outliers)\n\n\n\n\n\n\n\n\n\nbrfss_eliminated |&gt; \n  ggplot(aes(x = bmi))+\n  geom_histogram(aes(y = after_stat(density), fill = ..count..),bins = 30)+\n  facet_grid(~arthritis) +\n  theme_bw()\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n\n\n\n\n\n\n\n\nsex_subt&lt;-brfss_eliminated |&gt; \n  select(Sex,arthritis)\n\nactivity_subt&lt;-brfss_eliminated |&gt; \n  select(physical_activity,arthritis)\n\nage_subt&lt;-brfss_eliminated |&gt; \n  select(age_65_or_over,arthritis)\n\n\nsex_subt |&gt; \n  tbl_cross(row = Sex,\n            col = arthritis,\n            percent = \"column\") |&gt; \n  bold_labels()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\narthritis\n\nTotal\n\n\nYes\nNo\n\n\n\n\nSex\n\n\n\n\n\n\n\n\n    Female\n71,737 (63%)\n124,693 (53%)\n196,430 (56%)\n\n\n    Male\n42,144 (37%)\n109,887 (47%)\n152,031 (44%)\n\n\nTotal\n113,881 (100%)\n234,580 (100%)\n348,461 (100%)\n\n\n\n\n\n\n\n\nactivity_subt |&gt; \n  tbl_cross(row = physical_activity,\n            col = arthritis,\n            percent = \"column\") |&gt; \n  bold_labels()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\narthritis\n\nTotal\n\n\nYes\nNo\n\n\n\n\nphysical_activity\n\n\n\n\n\n\n\n\n    No\n36,915 (32%)\n51,544 (22%)\n88,459 (25%)\n\n\n    Yes\n76,966 (68%)\n183,036 (78%)\n260,002 (75%)\n\n\nTotal\n113,881 (100%)\n234,580 (100%)\n348,461 (100%)\n\n\n\n\n\n\n\n\nage_subt |&gt; \n  tbl_cross(row = age_65_or_over,\n            col = arthritis,\n            percent = \"column\") |&gt; \n  bold_labels()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\narthritis\n\nTotal\n\n\nYes\nNo\n\n\n\n\nage_65_or_over\n\n\n\n\n\n\n\n\n    Yes\n56,240 (49%)\n51,271 (22%)\n107,511 (31%)\n\n\n    No\n57,641 (51%)\n183,309 (78%)\n240,950 (69%)\n\n\nTotal\n113,881 (100%)\n234,580 (100%)\n348,461 (100%)\n\n\n\n\n\n\n\n\nbrfss_eliminated %&gt;% \n  count(Sex, arthritis) %&gt;%\n  ggplot(aes(x = Sex, \n             y = arthritis)) +\n  geom_tile(aes(fill=n)) +\n  labs(x = \"Sex\",\n       y = \"Arthritis\",\n       fill = \"Count\") +\n  scale_fill_distiller(palette = \"RdPu\") +\n  theme_classic(base_size = 16)"
  },
  {
    "objectID": "stat_modelling.html#observations",
    "href": "stat_modelling.html#observations",
    "title": "stat_modelling",
    "section": "Observations",
    "text": "Observations\nThe box plots show higher median BMI in the arthritis group with just fewer outliers after filtering. Histograms show a right skewed distribution where as contingency tables suggest higher arthritis prevalence among females (63%), those aged 65 or over (49% vs 22%), and those with no physical activity. the distribution of BMI as a variable indicates skewness and presence of outliers. This variables will be transformed prior to analysis."
  },
  {
    "objectID": "stat_modelling.html#data-dictionary",
    "href": "stat_modelling.html#data-dictionary",
    "title": "stat_modelling",
    "section": "Data dictionary",
    "text": "Data dictionary\n\nArthritis: A binary outcome variable taking the values of Yes(1), and No(2).\nPhysical activity: A binary explanatory variable taking the values of Yes(1), or No(2) to indicate whether an individual is exercising or not respectively.\nAge 65 or over: A binary explanatory variable taking the values of Yes(1), or No(2) to indicate whether or not the individual is above or below 65 years of age respectively.\nSex: A binary explanatory variable taking the values of Female(1), or Male(2) to indicate whether or not the individual is Female or Male respectively.\nBMI: A numeric continuous variable showing the Body Mass Index (BMI) for an individual."
  },
  {
    "objectID": "stat_modelling.html#data-transformation",
    "href": "stat_modelling.html#data-transformation",
    "title": "stat_modelling",
    "section": "Data Transformation",
    "text": "Data Transformation\nObservations from the visual inspection of the data, show that the bmi variable is not normally distributed, as the histograms and Q-Q plots have longer tails to the right, and data points deviating from red line respectively. Below, we are methods to transform the variable (bmi) using the three methods.\n\nLog transformation\n\nlean_brfss |&gt; \n  ggplot(aes(x = log(bmi)))+\n  geom_histogram(aes(y = after_stat(density), \n                     fill = ..count..))+\n  #geom_density(aes(y = after_stat(density)), color = \"red\", linewidth = 1) +\n  geom_vline(xintercept = mean(log(lean_brfss$bmi)), \n             color = \"purple\", linewidth = 0.8)+\n  geom_vline(xintercept = median(log(lean_brfss$bmi)), \n             color = \"orange\", linewidth = 0.8)+\n  facet_grid(~arthritis) +\n  theme_bw()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\nSquare root transformation\n\nlean_brfss |&gt; \n  ggplot(aes(x = sqrt(bmi)))+\n  geom_histogram(aes(y = after_stat(density), \n                     fill = ..count..))+\n  #geom_density(aes(y = after_stat(density)), color = \"red\", linewidth = 1) +\n  geom_vline(xintercept = mean(sqrt(lean_brfss$bmi)), \n             color = \"purple\", linewidth = 0.8)+\n  geom_vline(xintercept = median(sqrt(lean_brfss$bmi)), \n             color = \"orange\", linewidth = 0.8)+\n  facet_grid(~arthritis) +\n  theme_bw()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\nSquare transformation\n\nlean_brfss |&gt; \n  ggplot(aes(x = (bmi^2)))+\n  geom_histogram(aes(y = after_stat(density), \n                     fill = ..count..))+\n  #geom_density(aes(y = after_stat(density)), color = \"red\", linewidth = 1) +\n  geom_vline(xintercept = mean((lean_brfss$bmi^2)), \n             color = \"purple\", linewidth = 0.8)+\n  geom_vline(xintercept = median((lean_brfss$bmi^2)), \n             color = \"orange\", linewidth = 0.8)+\n  facet_grid(~arthritis) +\n  theme_bw()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nFrom the above outputs we can see that all methods of data transformation have not been able to result in normal distribution of bmi. Since graphic visualizations of tests are necessary but not sufficient we have further used the Shapiro test of significance below."
  },
  {
    "objectID": "stat_modelling.html#test-of-signifcance",
    "href": "stat_modelling.html#test-of-signifcance",
    "title": "stat_modelling",
    "section": "Test of signifcance",
    "text": "Test of signifcance\nUsing a sample of 4999 observations from the main dataset, we test for normality using the Shapiro Wilks test.\n\nset.seed(1234) # for reproducibility\nbmi_sample&lt;-lean_brfss |&gt;\n  select(bmi) |&gt; \n  drop_na(bmi) |&gt; \n  sample_n(4999) \n\nbmi_sample |&gt; \n  pull(bmi) |&gt; \n  shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  pull(bmi_sample, bmi)\nW = 0.91633, p-value &lt; 2.2e-16\n\n\nThe test results from the test show that firstly, the test statistic of 0.69345 is far from 1, indicating the deviation from the mean. The p-value is significant, is extremely low showing that there is strong evidence to reject the null hypothesis, and conclude that data for bmi is not coming from a normally distributed population.\nAll the assumptions except for chi-square test of independence have not been met for both the graphics, and significance tests. Since parametric methods such as students t tests, anova require data to be normally distributed, we are going to use non-parametric methods to analyse the dataset even as these methods have less power."
  },
  {
    "objectID": "stat_modelling.html#assessing-associations",
    "href": "stat_modelling.html#assessing-associations",
    "title": "stat_modelling",
    "section": "Assessing associations",
    "text": "Assessing associations\n\nsex_subt |&gt; \n  table() |&gt; \n  chisq.test()\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table(sex_subt)\nX-squared = 3016, df = 1, p-value &lt; 2.2e-16\n\n\n\nage_subt |&gt; \n  table() |&gt; \n  chisq.test()\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table(age_subt)\nX-squared = 27231, df = 1, p-value &lt; 2.2e-16\n\n\n\nactivity_subt |&gt; \n  table() |&gt; \n  chisq.test()\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table(activity_subt)\nX-squared = 4413, df = 1, p-value &lt; 2.2e-16\n\n\n\nwilcox.test(bmi~arthritis,data=brfss_eliminated,conf.int=T)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  bmi by arthritis\nW = 1.5633e+10, p-value &lt; 2.2e-16\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n 1.489961 1.559952\nsample estimates:\ndifference in location \n              1.520008"
  },
  {
    "objectID": "stat_modelling.html#assessing-relative-risk-rr",
    "href": "stat_modelling.html#assessing-relative-risk-rr",
    "title": "stat_modelling",
    "section": "Assessing relative risk (RR)",
    "text": "Assessing relative risk (RR)\n\nsex_subt |&gt; \n  table() |&gt; \n  epitools::epitab(method=\"riskratio\")\n\n$tab\n        arthritis\nSex        Yes        p0     No        p1 riskratio    lower    upper p.value\n  Female 71737 0.3652039 124693 0.6347961  1.000000       NA       NA      NA\n  Male   42144 0.2772066 109887 0.7227934  1.138623 1.133424 1.143845       0\n\n$measure\n[1] \"wald\"\n\n$conf.level\n[1] 0.95\n\n$pvalue\n[1] \"fisher.exact\"\n\n\n\nage_subt |&gt; \n  table() |&gt; \n  epitools::epitab(method=\"riskratio\")\n\n$tab\n              arthritis\nage_65_or_over   Yes        p0     No        p1 riskratio    lower    upper\n           Yes 56240 0.5231093  51271 0.4768907  1.000000       NA       NA\n           No  57641 0.2392239 183309 0.7607761  1.595284 1.584712 1.605926\n              arthritis\nage_65_or_over p.value\n           Yes      NA\n           No        0\n\n$measure\n[1] \"wald\"\n\n$conf.level\n[1] 0.95\n\n$pvalue\n[1] \"fisher.exact\"\n\n\n\nactivity_subt |&gt; \n  table()|&gt; \n  epitools::epitab(method=\"riskratio\")\n\n$tab\n                 arthritis\nphysical_activity   Yes        p0     No        p1 riskratio  lower    upper\n              No  36915 0.4173120  51544 0.5826880  1.000000     NA       NA\n              Yes 76966 0.2960208 183036 0.7039792  1.208158 1.2008 1.215561\n                 arthritis\nphysical_activity p.value\n              No       NA\n              Yes       0\n\n$measure\n[1] \"wald\"\n\n$conf.level\n[1] 0.95\n\n$pvalue\n[1] \"fisher.exact\"\n\n\n\nmodel_data&lt;-brfss_eliminated |&gt; \n  select(arthritis,bmi,age_65_or_over,Sex,physical_activity)\n\nmodel_lr&lt;-glm(arthritis~.,data = model_data,family = binomial(\"logit\"))\nsummary(model_lr)\n\n\nCall:\nglm(formula = arthritis ~ ., family = binomial(\"logit\"), data = model_data)\n\nCoefficients:\n                       Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)           1.2621793  0.0228869   55.15   &lt;2e-16 ***\nbmi                  -0.0660347  0.0007623  -86.62   &lt;2e-16 ***\nage_65_or_overNo      1.2707663  0.0079701  159.44   &lt;2e-16 ***\nSexMale               0.4330701  0.0078421   55.22   &lt;2e-16 ***\nphysical_activityYes  0.3712731  0.0085998   43.17   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 440383  on 348460  degrees of freedom\nResidual deviance: 400874  on 348456  degrees of freedom\nAIC: 400884\n\nNumber of Fisher Scoring iterations: 4\n\nmodel_lr |&gt; \n  tbl_regression(exponentiate = TRUE, conf.int = TRUE) |&gt; \n  add_global_p() |&gt; \n  bold_labels() |&gt; \n  add_glance_table(include = c(nobs,BIC,AIC,logLik))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR\n95% CI\np-value\n\n\n\n\nbmi\n0.94\n0.93, 0.94\n&lt;0.001\n\n\nage_65_or_over\n\n\n\n\n&lt;0.001\n\n\n    Yes\n—\n—\n\n\n\n\n    No\n3.56\n3.51, 3.62\n\n\n\n\nSex\n\n\n\n\n&lt;0.001\n\n\n    Female\n—\n—\n\n\n\n\n    Male\n1.54\n1.52, 1.57\n\n\n\n\nphysical_activity\n\n\n\n\n&lt;0.001\n\n\n    No\n—\n—\n\n\n\n\n    Yes\n1.45\n1.43, 1.47\n\n\n\n\nNo. Obs.\n348,461\n\n\n\n\n\n\nBIC\n400,938\n\n\n\n\n\n\nAIC\n400,884\n\n\n\n\n\n\nLog-likelihood\n-200,437\n\n\n\n\n\n\n\nAbbreviations: CI = Confidence Interval, OR = Odds Ratio"
  },
  {
    "objectID": "stat_modelling.html#sex-and-arthritis",
    "href": "stat_modelling.html#sex-and-arthritis",
    "title": "stat_modelling",
    "section": "1. Sex, and Arthritis",
    "text": "1. Sex, and Arthritis\n\nChi-square test of indeoendence shows a statistically significant relationship between sex and arthritis (X-squared = 27231 = 3016, df = 1, p&lt;0.001), indicating that arthritis prevalence is different in males and females.\nRisk Ratio: Males have a lower risk of arthritis comppared to females (RR = 0.86, 95% CI: 0.86-0.88, p&lt;0.001). This means that the risk of arthritis in males is about 14% lower than in females.\nLogistic regression: Adjusted OR = 1.54 (95% CI: 1.52-1.57), p&lt;0.001. This means that after adjusting for BMI, age, and physical activity, females have 54% higher odds of having arthritis compared to males. The 95% confidence interval does not include 1, and extremely narrow indicating that the estimate is quite precise."
  },
  {
    "objectID": "stat_modelling.html#age-and-arthritis",
    "href": "stat_modelling.html#age-and-arthritis",
    "title": "stat_modelling",
    "section": "2. Age and Arthritis",
    "text": "2. Age and Arthritis\n\nChi-square test results show that there is a strong evidence of association between age (65and over) and arthritis (X-squared = 27231, df = 1, p &lt; 0.001). The p-value is less than 0.05, we therefore reject the null hypothesis, and conclude that the association between age and arthritis is not by chance.\nRisk Ratio: Individuals aged 65 or older have a relative risk of 2.18 (95% CI: 2.15-2.20, p &lt; 0.001), suggesting that they are more than twice as likely to have arthritis compared to individuals below 65.\n\nLogistic regression: Adjusted OR = 3.56 (95% CI: 3.51-3.62, p &lt; 0.001), showing that being 65 or older increases the odds of having arthritis by 3.5 times after adjusting for other variables. Again, the low p - value (less than 0.05) as well as the narrow confidence interval shows that the estimate is both significant and quite precise."
  },
  {
    "objectID": "stat_modelling.html#physical-activity-and-arthritis",
    "href": "stat_modelling.html#physical-activity-and-arthritis",
    "title": "stat_modelling",
    "section": "3. Physical Activity and Arthritis",
    "text": "3. Physical Activity and Arthritis\n\nChi-square test: The association between physical activity, and arthritis is significant (X-squared = 4413, df = 1, p &lt; 0.001). Given the small p value (less than 0.05), we reject the null hypothesis, and conclude that the difference is unlikely to be due to random variation alone.\nRisk Ratio: Individuals who are physically inactive have higher risk of developing athritis (RR = 1.21, 95% CI: 1.20-1.22, p &lt; 0.001), showing that there is a 21% increased risk compared to physically active individuals.\nLogistic regression: With Adjusted OR = 1.45 (95% CI: 1.43-1.47, p &lt; 0.001). This means that physical inactivity increases the odds of arthritis by 45% holding other variables constant.The p value is less than 0.05 (given alpha level), and therefore significant. Note that the confidence interval is narrow, and does not include 1 making the estimate so certain and precise."
  },
  {
    "objectID": "stat_modelling.html#bmi-and-arthritis",
    "href": "stat_modelling.html#bmi-and-arthritis",
    "title": "stat_modelling",
    "section": "4. BMI and Arthritis",
    "text": "4. BMI and Arthritis\n\nWilcoxon Rank-Sum Test (Mann-Whitney U Test): Since BMI is numeric and skewed, this test is ideal because it is non-parametric. The p - value is less than 0.05 (0.001), and the confidence interval is narrow(W-Statistic 1.52, 95% CI:1.49-1.56, p &lt; 0.001) and does not include 0, we reject the null hypothesis, and conclude that individuals with arthritis tend to have higher BMI than those without arthritis.\nLogistic regression: The adjusted OR = 0.94 (95% CI: 0.93-0.94, p &lt; 0.001) suggests that there is a 6% decrease in the odds of arthritis for every unit of BMI gained. Even as this is significant the result is inverse relationship could suggest possible confounding in our model."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Portfolio",
    "section": "",
    "text": "Welcome to my portfolio! I am a David Mwale, a second-year MSc student in Data Science for Health and Social Care at the University of Edinburgh. I am using this platform to showcase my evolving journey in data science, in doing so intend to use data to to improve health and social care outcomes. Over the past 2 years, I have immersed my self in a continuous learning process, learning foundational concepts, unlearning, and re-learning through practical application. What I have learned, shaped by intensive course work, hands on projects, is much more than i will be able to showcase through this platform. I am experienced in Python, SQL, R programming, and health data analysis, with focus on reproducible research, and ethical data practices. Explore my work, and feel free to connect with me to discuss potential collaborations, role openings in data science, or insights."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "My Portfolio",
    "section": "",
    "text": "Welcome to my portfolio! I am a David Mwale, a second-year MSc student in Data Science for Health and Social Care at the University of Edinburgh. I am using this platform to showcase my evolving journey in data science, in doing so intend to use data to to improve health and social care outcomes. Over the past 2 years, I have immersed my self in a continuous learning process, learning foundational concepts, unlearning, and re-learning through practical application. What I have learned, shaped by intensive course work, hands on projects, is much more than i will be able to showcase through this platform. I am experienced in Python, SQL, R programming, and health data analysis, with focus on reproducible research, and ethical data practices. Explore my work, and feel free to connect with me to discuss potential collaborations, role openings in data science, or insights."
  },
  {
    "objectID": "index.html#my-projects",
    "href": "index.html#my-projects",
    "title": "My Portfolio",
    "section": "My Projects",
    "text": "My Projects\n\nData Wrangling in R : Cleaning and transforming datasets using tidyverse and other R packages like nanair, and janitor. A quick example is my work in the adjacent with the Iris dataset where i have cleaned the data, and handled missing values.\nData Visualization : Creating insightful visualizations with ggplot2 and interactive dashboards. Through out my projects in this portfolio, I use ggplot2 to make visualizations.\nStatistical Modelling for Epidemiology : Applying statistical models to analyze health data trends. In the stats modelling tab i apply one of the commonly used models - logistic regression to analyse health data, and using other packages like epitools and reportROC.\nMachine Learning with R : Building predictive models using tidymodels. A highlight of this project is under the Machine Learning tab where i use both logistic regression and Random Forest models to predict diabetes."
  },
  {
    "objectID": "index.html#skills",
    "href": "index.html#skills",
    "title": "My Portfolio",
    "section": "Skills",
    "text": "Skills\n\nLanguages\n\nR : Proficient in data manipulation, visualization and modelling with experience in reproducible workflows using RMarkdown, and Quarto.\nSQL : Skilled in querying health databases, optimizing joins, sub-queries, common table expressions(CTEs) using SQLite, MySQL, and PostgreSQL.\nPython: Experienced in pandas for data wrangling, matplotlib/seaborn for visualization.\nVersion Control: Git and Github for collaborative and reproducible research."
  },
  {
    "objectID": "cleaning.html",
    "href": "cleaning.html",
    "title": "Cleaning Iris dataset",
    "section": "",
    "text": "library(tidyverse)\nlibrary(messy) # for creating messy data\nlibrary(naniar) # for assessing missing values\nlibrary(janitor) # Data cleaning\nlibrary(gt) #generating tables\nlibrary(gtExtras)\nlibrary(cowplot)"
  },
  {
    "objectID": "cleaning.html#loading-of-libraries",
    "href": "cleaning.html#loading-of-libraries",
    "title": "Cleaning Iris dataset",
    "section": "",
    "text": "library(tidyverse)\nlibrary(messy) # for creating messy data\nlibrary(naniar) # for assessing missing values\nlibrary(janitor) # Data cleaning\nlibrary(gt) #generating tables\nlibrary(gtExtras)\nlibrary(cowplot)"
  },
  {
    "objectID": "cleaning.html#the-iris-dataset",
    "href": "cleaning.html#the-iris-dataset",
    "title": "Cleaning Iris dataset",
    "section": "The Iris dataset",
    "text": "The Iris dataset\nThe Iris dataset is one of the most famous datasets in statistics and machine learning. It was first introduced by the British biologist and statistician Ronald Fisher in 1936 in his paper “The use of multiple measurements in taxonomic problems.” The dataset consists of 150 samples of iris flowers from three different species: Setosa, Versicolor, and Virginica. Each sample includes four features/columns/variables: sepal length, sepal width, petal length, and petal width\n\nData cleaning & EDA\nThe explorations that I will conduct in this document will involve the following:\n\nMessy column names\nImproper variable types\nInvalid or inconsistent values\nMissing values\nNon-standard data formats"
  },
  {
    "objectID": "cleaning.html#creating-a-messy-dataset",
    "href": "cleaning.html#creating-a-messy-dataset",
    "title": "Cleaning Iris dataset",
    "section": "Creating a messy dataset",
    "text": "Creating a messy dataset\n\nset.seed(123456)\nmessy_iris&lt;-messy(iris)\nmessy_iris %&gt;%\n  head() |&gt; \n  gt()\n\n\n\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n\n5.1\n3.5\n1.4\nNA\nsetosa\n\n\n4.9\n3\nNA\n0.2\ns$etosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\nNA\n3.1\n1.5\n0.2\nNA\n\n\n5\n3.6\n1.4\n0.2\nsetosa\n\n\n5.4\n3.9\n1.7\n0.4\nNA\n\n\n\n\n\n\n\n\nKey observations\n\nWe can see that the column names are separated by “.” and are not in lower case. we are going to convert these to lower snake_case.\nEven before we search for missing values, we can note that the dataset has missing values\nFinally we can also see that the species column has more that three variations of the setosa.\n\n\n\nUnderstanding the dataset\nNow, we are going to run different codes, just to understand our dataset.\n\nmessy_iris |&gt; #Checking the dimensions of the data (The data has 150 rows, and 5 columns)\n  dim()\n\n[1] 150   5\n\n\n\nmessy_iris %&gt;% # Taking a quick look at our dataset\n  glimpse()\n\nRows: 150\nColumns: 5\n$ Sepal.Length &lt;chr&gt; \"5.1\", \"4.9\", \"4.7\", NA, \"5\", \"5.4 \", \"4.6\", \"5\", \"4.4\", …\n$ Sepal.Width  &lt;chr&gt; \"3.5\", \"3\", \"3.2\", \"3.1 \", \"3.6\", \"3.9\", \"3.4\", \"3.4\", \"2…\n$ Petal.Length &lt;chr&gt; \"1.4\", NA, \"1.3\", \"1.5 \", \"1.4\", \"1.7\", \"1.4\", \"1.5\", \"1.…\n$ Petal.Width  &lt;chr&gt; NA, \"0.2\", \"0.2\", \"0.2\", \"0.2 \", \"0.4\", \"0.3\", \"0.2\", NA,…\n$ Species      &lt;chr&gt; \"setosa\", \"s$etosa\", \"setosa\", NA, \"setosa\", NA, \"setosa\"…\n\n\nIt’s good to understand our data. We have also noted that the data type for all the columns is character (chr) structure. This can limit certain operations that require our data to be in numeric or categorical (factor) form. We are also going to fix this.\n\nmessy_iris %&gt;% # Understanding the column names of the dataset\n  colnames()\n\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  \"Species\"     \n\n\n\nmessy_iris %&gt;% # Checking for unique values of Species. \n  select(Species) %&gt;% \n  distinct()\n\n          Species\n1          setosa\n2         s$etosa\n3            &lt;NA&gt;\n4         set)osa\n5        s+et&osa\n6          SETOSA\n7         se+tosa\n8         set_osa\n9         s@etosa\n10       s)eto!sa\n11        $setosa\n12        setos%a\n13        set-osa\n14        *setosa\n15       s^e*tosa\n16        s)etosa\n17        (setosa\n18        setos.a\n19        se(tosa\n20        setosa \n21      se#t#o)sa\n22        s-etosa\n23        s_etosa\n24        s&etosa\n25        s^etosa\n26        seto^sa\n27    ^set.o(s-a \n28    ver!sicolor\n29   ver)sicolor \n30   vers$icolor \n31   ve%rsicol!or\n32    ver$sicolor\n33    versic(olor\n34    versi(color\n35    versi-color\n36     versicolor\n37    versicol(or\n38    *versicolor\n39    versic+olor\n40   versi_co%lor\n41    VERS!ICOLOR\n42   versi$c$olor\n43   versi%c%olor\n44    versico&lor\n45   ve^rsicolor \n46    ^versicolor\n47    ve.rsicolor\n48   ver#sico!lor\n49  $v%ersicolo_r\n50    versicol#or\n51   ve@rs(icolor\n52    versicolor \n53   ve@rsicolo%r\n54    vers&icolor\n55  v_e)rsicolo)r\n56  ve(rsi.col*or\n57    versico)lor\n58   %versicolo#r\n59  versi&col#o!r\n60    versic^olor\n61  *versico!lo.r\n62   &vers)icolor\n63    ver^sicolor\n64   ver#sicolor \n65 ve_rs#ic-olo$r\n66   vers.icolor \n67      virginica\n68     @virginica\n69    virginic@a \n70  v-i*rg%i#nica\n71     virgin*ica\n72     virgi*nica\n73     virgin&ica\n74   v%irgini%ca \n75     virgini+ca\n76     virgini)ca\n77     v.irginica\n78   virgi(nic-a \n79     -virginica\n80    virg*inica \n81     virginic$a\n82    vir&gini(ca\n83     v-irginica\n84     virgi@nica\n85     &virginica\n86     virginica \n87   vir@ginic)a \n88    #virgin(ica\n89    virg(inica \n90   virg_i%nic^a\n91     virginic.a\n\n\nThe dataset is supposed to have three different species of the flower namely; setosa, viginica, and versicolor. However, we can quickly note from code output that we have over 68 different variations of these species. Again, we are going to fix this too!!"
  },
  {
    "objectID": "cleaning.html#data-cleaning-process",
    "href": "cleaning.html#data-cleaning-process",
    "title": "Cleaning Iris dataset",
    "section": "Data Cleaning process",
    "text": "Data Cleaning process\nFirstly, we are going to take the messy dataset, and load it into the clean_iris data object as this is what will finally house our clean dataset. Immediately, we will start cleaning by working on the column names using the clean_names() function from the janitor package.\n\nclean_iris&lt;-messy_iris %&gt;% \n  clean_names()\nclean_iris |&gt; \n  head(10)\n\n   sepal_length sepal_width petal_length petal_width species\n1           5.1         3.5          1.4        &lt;NA&gt;  setosa\n2           4.9           3         &lt;NA&gt;         0.2 s$etosa\n3           4.7         3.2          1.3         0.2  setosa\n4          &lt;NA&gt;        3.1          1.5          0.2    &lt;NA&gt;\n5             5         3.6          1.4        0.2   setosa\n6          5.4          3.9          1.7         0.4    &lt;NA&gt;\n7           4.6         3.4          1.4         0.3  setosa\n8             5         3.4          1.5         0.2  setosa\n9           4.4         2.9         1.4         &lt;NA&gt;  setosa\n10          4.9         3.1         &lt;NA&gt;        0.1  set)osa\n\n\nNote that our column names are now in lower case using the snake_case format. The next thing that we are going to do is ensure that the species column only has the three different values.\n\nhead(clean_iris$species)\n\n[1] \"setosa\"  \"s$etosa\" \"setosa\"  NA        \"setosa\"  NA       \n\n\n\nbad_setosa &lt;- c( \"setos)a\", \"setosa \", \"setosa\",\"setosa\", \"setosa\", \"setosa \", \"seto_sa\",  \"s&etosa\",  \"setosa\", \"SETOSA\", \"setosa\", \"se(tosa\", \"setosa\",\"setosa\", \"setosa\",\"setosa\",\"*setosa\",\"set_osa\",\"setosa\",         \"se@tosa\",\"setosa\", \"(s_etos.a\",      \"set(osa\",\"setos$a\",\"seto-s(a\",\"(SETOSA\",\"setosa \", \"s-eto%sa\",       \"setosa\",\"SETOSA\", \"seto.sa\",\"setosa\",\"setos^a\", \"setosa\",\"set$osa\", \"setosa\", \"se+tosa\",\"seto*sa\",        \"S)ETOSA\",\"setos*a\", \"setosa\",\"set!osa\",\"setosa\",\"setosa\",\"s@et#osa \",\"setosa\",\"setosa\")\n\n\nbad_versicolor&lt;-c(\"versic(olor\",\"ver@sicolor\",\"versico_lor\",\"ve#rsicolor\",\"versicolor\",          \"versico@lor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"vers_i%c#ol%or\", \"V*ERSICOLOR\",\"ver!sicolor\",\"+versicolo^r\",\"versicolor\",\"versico)l^or\",\"versicol^or\",\"ve&rsicolor\",\"versicolor\",\"$vers+icolor\",\"versicolor \",\")versicolor\", \"versicolor\",\"versicolor\",\"versicolor\",\"versicolor \",\"ver&sicolor \",\"versico(lo$r\",\"versi_color\",\"versicolor\",\"vers-ic.ol%o&r\", \"versicolor\",\"versicolor\",     \"versicolor\",\"*versicolor\",\"versicolor\",\"versicol!or\",\"&versicolor\",\"%versicol%or \",  \"v%ersicolor\",\"v+ersicolor\")\n\n\nbad_virginica &lt;- c(\"virginica\",\"vir!ginica\",\"virginica\",\"VIRGINICA\",\"virginica\",\"virginica\",     \n\"virginica\",\"virg^inica\",\"virginica\",\"$virg(inica\",\"virginica\",\"virginica \",\"virginica\",     \"virginica\",\"virgini+ca\",\"vir-ginica\", \"virginica\",\"virginica\",\"virgin!ica\",\"virginica\",   \".virginic#a\",\"virginica\",\"virginic_a\",\"virginica\",\"v(irgi$nica\",\"virginica\",\"virginic#a\",     \"vir.gini@ca\",\"virginica \",\"v#irgini(ca\", \"virginica\",\"virginica\",\"virginica\",\"virginica\",      \"virgi^nica\",\"virginica\",\"virginica\",\"virginica\",\"VIRGINICA\",\"virginica\",\"virginica\")\n\nThe code below, is going to replace bad species with the right value using dplyr case_when function\n\nclean_iris&lt;-clean_iris %&gt;% \n   mutate(species_clean = case_when(species %in% bad_setosa ~ \"setosa\",\n                                    species %in% bad_versicolor ~ \"versicolor\",\n                                    species %in% bad_virginica ~ \"virginica\"))\n\nunique(clean_iris$species_clean)\n\n[1] \"setosa\"     NA           \"versicolor\" \"virginica\" \n\n\n\nConverting variables\n\nclean_iris |&gt; \n  mutate(across(c(sepal_length,\n                  sepal_width,\n                  petal_length,\n                  petal_width),as.numeric),\n         species_clean=factor(species_clean)) |&gt; \n  glimpse()\n\nRows: 150\nColumns: 6\n$ sepal_length  &lt;dbl&gt; 5.1, 4.9, 4.7, NA, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ sepal_width   &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3…\n$ petal_length  &lt;dbl&gt; 1.4, NA, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, NA, 1.5, 1.6…\n$ petal_width   &lt;dbl&gt; NA, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, NA, 0.1, 0.2, 0.2…\n$ species       &lt;chr&gt; \"setosa\", \"s$etosa\", \"setosa\", NA, \"setosa\", NA, \"setosa…\n$ species_clean &lt;fct&gt; setosa, NA, setosa, NA, setosa, NA, setosa, setosa, seto…\n\n\nAfter conversion, we now have the double precision, factor, and character data types. This will be important in our analysis."
  },
  {
    "objectID": "cleaning.html#missing-values",
    "href": "cleaning.html#missing-values",
    "title": "Cleaning Iris dataset",
    "section": "Missing values",
    "text": "Missing values\nThere are many ways of working with missing values including methods such as listwise deletion, pairwise deletion, imputation etc. In this section we are going to use imputation by employing a package; missForest, which uses random forest to train data of observed values of data matrix to predict missing values.\n\n#install.packages(\"missForest\")\nlibrary(missForest)\n\niris_impute&lt;-clean_iris |&gt; \n  select(-species) |&gt; \n  mutate(across(c(sepal_length,\n                  sepal_width,\n                  petal_length,\n                  petal_width), as.numeric),\n         species_clean = as.factor(species_clean))\n\n\niris_imputed&lt;-missForest(iris_impute,xtrue = ,maxiter = 10,ntree = 100,verbose = FALSE)\n\ndf_imputed&lt;-iris_imputed$ximp\n\ndf_imputed %&gt;% \n  miss_var_summary() %&gt;% \n  gt() \n\n\n\n\n\n\n\nvariable\nn_miss\npct_miss\n\n\n\n\nsepal_length\n0\n0\n\n\nsepal_width\n0\n0\n\n\npetal_length\n0\n0\n\n\npetal_width\n0\n0\n\n\nspecies_clean\n0\n0\n\n\n\n\n\n\n\nEven though imputing datasets (multiple imputation) is better than methods like list wise deletion, along with it comes ethical implications especially for identity data.\n\niris_imputed$OOBerror\n\n     NRMSE        PFC \n0.13960904 0.01587302 \n\n\nThe error rates for both the categorical and numerical values are relatively low."
  },
  {
    "objectID": "cleaning.html#exploratory-data-analysis-eda",
    "href": "cleaning.html#exploratory-data-analysis-eda",
    "title": "Cleaning Iris dataset",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\nIn this section we will understand our data further using graphics to see the distribution of different variables. We will use boxplots, q-q plots, and histograms to assess distribution of variables.\n\nplot_sl_1&lt;-df_imputed |&gt; \n  ggplot(aes(x = sepal_length))+\n  geom_histogram()+\n  theme_bw() +\n  labs(title = \"Histogram - Imputed data\")\niris_sp1&lt;-iris |&gt; \n  ggplot(aes(x = Sepal.Length))+\n  geom_histogram()+\n  theme_bw()+\n  labs(title = \"Histogram - original data\")\n\nplot_sl_2&lt;-df_imputed |&gt; \n  ggplot(aes(sample = sepal_length))+\n  stat_qq()+\n  stat_qq_line(color = \"red\")+\n  theme_bw() +\n  labs(title = \"Q-Q plot for imputed data\")\n\niris_sl_2&lt;-iris |&gt; \n  ggplot(aes(sample = Sepal.Length))+\n  stat_qq()+\n  stat_qq_line(color = \"red\")+\n  theme_bw() +\n  labs(title = \"Q-Q Plot original data\")\n\ncowplot::plot_grid(plot_sl_1, iris_sp1,plot_sl_2,iris_sl_2, ncol = 2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "cleaning.html#visualisation",
    "href": "cleaning.html#visualisation",
    "title": "Cleaning Iris dataset",
    "section": "VISUALISATION",
    "text": "VISUALISATION\n\ndf_imputed |&gt; \n  ggplot(aes(x = sepal_length,y = petal_width))+\n  geom_point(aes(colour = species_clean, size = petal_length), alpha = 0.5) +\n  scale_color_manual(values = c(\"#00AFBB\", \"#e7b800\",\"#FC4E07\"))+\n  scale_size(range = c(0.5, 12)) +\n  theme_bw()+\n  labs(title = \"Plot showing Petal width against Petal length\")"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "My name is David Mwale. I am a second-year Masters student at the University of Edinburgh, studying Data Science for Health and Social Care. My interests include Statistical Modelling, Data Cleaning, Visualization, building data pipelines for automation, and Machine Learning. I have over 10 years experience working with data in the fields of health (SRH), agriculture, livelihoods, and nutrition. Currently I am working with Family Planning Association of Malawi as an M&E Lead."
  },
  {
    "objectID": "about.html#education-experience",
    "href": "about.html#education-experience",
    "title": "About Me",
    "section": "Education & Experience",
    "text": "Education & Experience\n\nMSc in Data Science for Health and Social care | University of Edinburgh| 2023- present\nDiploma in Monitoring and Evaluation | The Catholic University of Malawi | 2018-20\nBSc Agricultural Economics | University of Malawi | 2011-15\nRUsers Malawi Lead organiser | Lilongwe, Malawi | 2024- present"
  },
  {
    "objectID": "about.html#business-intelligence",
    "href": "about.html#business-intelligence",
    "title": "About Me",
    "section": "Business Intelligence ",
    "text": "Business Intelligence \n\nPowerBI\n\n\nContact \n\nEmail: d.mwale@sms.ed.ac.uk\nPhone: +265 993 627 367\nLinkedIn"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Reach out for collaborations, R workshops, or data science opportunities:\n\nEmail: d.mwale@sms.ed.ac.uk\nPhone: +265993627367\nGitHub: mwaledaveed99\n\nI am willing to take full-time, part-time, and remote data science related roles."
  },
  {
    "objectID": "contact.html#lets-get-in-touch",
    "href": "contact.html#lets-get-in-touch",
    "title": "Contact",
    "section": "",
    "text": "Reach out for collaborations, R workshops, or data science opportunities:\n\nEmail: d.mwale@sms.ed.ac.uk\nPhone: +265993627367\nGitHub: mwaledaveed99\n\nI am willing to take full-time, part-time, and remote data science related roles."
  },
  {
    "objectID": "nhanes_ml_project.html",
    "href": "nhanes_ml_project.html",
    "title": "MLearning",
    "section": "",
    "text": "In this project we will use the NHANES dataset to predict diabetes given the available risk factors. The National Health ans Nutrition Survey is a a program in the US designed to assess the health and nutritional status pf adults, and children in the US. The data includes demographic, socio-economic, dietary, and health-related information.\n\n\n\n\n\nWe are going to save the dataset into the nhanes_df object to maintain the original dataset intact.\n\nnhanes_df&lt;-NHANES |&gt; \n  select(Diabetes,DirectChol,BMI,MaritalStatus,Age,Gender) |&gt;\n  drop_na() |&gt; \n  clean_names()\n\n# Changing the levels for appropriate analysis\nnhanes_df&lt;-nhanes_df |&gt; \n  mutate(diabetes=factor(diabetes, \n                         levels = c(\"Yes\", \"No\"))) |&gt; \n           glimpse()\n\nRows: 6,786\nColumns: 6\n$ diabetes       &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, No, No,…\n$ direct_chol    &lt;dbl&gt; 1.29, 1.29, 1.29, 1.16, 2.12, 2.12, 2.12, 0.67, 0.96, 1…\n$ bmi            &lt;dbl&gt; 32.22, 32.22, 32.22, 30.57, 27.24, 27.24, 27.24, 23.67,…\n$ marital_status &lt;fct&gt; Married, Married, Married, LivePartner, Married, Marrie…\n$ age            &lt;int&gt; 34, 34, 34, 49, 45, 45, 45, 66, 58, 54, 58, 50, 33, 60,…\n$ gender         &lt;fct&gt; male, male, male, female, female, female, female, male,…\n\n\nIn the code belwo, we are splitting our data into training and testing sets (0.8, 0.2) and stratify by the target variable so that we do not end up having all the data from the target variable.\n\nset.seed(123) #For reproducubility\nml_split&lt;-initial_split(nhanes_df,\n                        prop = 0.8,\n                        strata = diabetes)\n\nml_training&lt;-ml_split |&gt; \n  training()\n\nml_test&lt;-ml_split |&gt;\n  testing()\n\nWe are going to specify 2 models, logistic regression, and Random Forest.\n\nlr_model&lt;-logistic_reg() |&gt; \n  set_engine(\"glm\") |&gt; \n  set_mode(\"classification\") \n\n\nml_training |&gt; \n  select_if(is.numeric) |&gt; \n  cor()\n\n            direct_chol         bmi        age\ndirect_chol   1.0000000 -0.34679048 0.10827307\nbmi          -0.3467905  1.00000000 0.03385215\nage           0.1082731  0.03385215 1.00000000\n\n\nUsing the hypothetical threshold of 0.8, we can conclude that the predictors are not collerated. In the code below, we are going to fit both models using the fit function. After which we are going to collect and combine predictions, and load them.\nIn the code below we are going to specify a recipe object after which we will add steps for engineering our features (feature engineering). The steps are to preprocess the data into a form that will allegedly improve our analysis.\n\nset.seed(123)\nlr_recipe&lt;-recipe(diabetes~.,data = ml_training) |&gt;\n  step_log(all_numeric()) |&gt; \n  step_normalize(all_numeric()) |&gt; #Centering and scaling\n  step_dummy(all_nominal(), -all_outcomes())\n\n\nlr_worflow&lt;-workflow() |&gt; \n  add_model(lr_model) |&gt; \n  add_recipe(lr_recipe)\n\n\nset.seed(123)\nlr_worflow_fit&lt;-lr_worflow |&gt; \n  last_fit(split = ml_split)\n\nlr_worflow_fit |&gt; \n  collect_metrics()\n\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary        0.909  Preprocessor1_Model1\n2 roc_auc     binary        0.790  Preprocessor1_Model1\n3 brier_class binary        0.0743 Preprocessor1_Model1\n\nlr_resultss&lt;-lr_worflow_fit |&gt; \n  collect_predictions()\n\nlr_resultss\n\n# A tibble: 1,358 × 7\n   .pred_class .pred_Yes .pred_No id                .row diabetes .config       \n   &lt;fct&gt;           &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;            &lt;int&gt; &lt;fct&gt;    &lt;chr&gt;         \n 1 No             0.0774    0.923 train/test split     4 No       Preprocessor1…\n 2 No             0.0941    0.906 train/test split    10 No       Preprocessor1…\n 3 No             0.103     0.897 train/test split    11 No       Preprocessor1…\n 4 No             0.101     0.899 train/test split    12 No       Preprocessor1…\n 5 No             0.112     0.888 train/test split    14 No       Preprocessor1…\n 6 No             0.0235    0.976 train/test split    15 No       Preprocessor1…\n 7 No             0.207     0.793 train/test split    19 No       Preprocessor1…\n 8 No             0.131     0.869 train/test split    22 Yes      Preprocessor1…\n 9 No             0.122     0.878 train/test split    24 Yes      Preprocessor1…\n10 No             0.0877    0.912 train/test split    28 No       Preprocessor1…\n# ℹ 1,348 more rows"
  },
  {
    "objectID": "nhanes_ml_project.html#introduction",
    "href": "nhanes_ml_project.html#introduction",
    "title": "MLearning",
    "section": "",
    "text": "In this project we will use the NHANES dataset to predict diabetes given the available risk factors. The National Health ans Nutrition Survey is a a program in the US designed to assess the health and nutritional status pf adults, and children in the US. The data includes demographic, socio-economic, dietary, and health-related information.\n\n\n\n\n\nWe are going to save the dataset into the nhanes_df object to maintain the original dataset intact.\n\nnhanes_df&lt;-NHANES |&gt; \n  select(Diabetes,DirectChol,BMI,MaritalStatus,Age,Gender) |&gt;\n  drop_na() |&gt; \n  clean_names()\n\n# Changing the levels for appropriate analysis\nnhanes_df&lt;-nhanes_df |&gt; \n  mutate(diabetes=factor(diabetes, \n                         levels = c(\"Yes\", \"No\"))) |&gt; \n           glimpse()\n\nRows: 6,786\nColumns: 6\n$ diabetes       &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, No, No,…\n$ direct_chol    &lt;dbl&gt; 1.29, 1.29, 1.29, 1.16, 2.12, 2.12, 2.12, 0.67, 0.96, 1…\n$ bmi            &lt;dbl&gt; 32.22, 32.22, 32.22, 30.57, 27.24, 27.24, 27.24, 23.67,…\n$ marital_status &lt;fct&gt; Married, Married, Married, LivePartner, Married, Marrie…\n$ age            &lt;int&gt; 34, 34, 34, 49, 45, 45, 45, 66, 58, 54, 58, 50, 33, 60,…\n$ gender         &lt;fct&gt; male, male, male, female, female, female, female, male,…\n\n\nIn the code belwo, we are splitting our data into training and testing sets (0.8, 0.2) and stratify by the target variable so that we do not end up having all the data from the target variable.\n\nset.seed(123) #For reproducubility\nml_split&lt;-initial_split(nhanes_df,\n                        prop = 0.8,\n                        strata = diabetes)\n\nml_training&lt;-ml_split |&gt; \n  training()\n\nml_test&lt;-ml_split |&gt;\n  testing()\n\nWe are going to specify 2 models, logistic regression, and Random Forest.\n\nlr_model&lt;-logistic_reg() |&gt; \n  set_engine(\"glm\") |&gt; \n  set_mode(\"classification\") \n\n\nml_training |&gt; \n  select_if(is.numeric) |&gt; \n  cor()\n\n            direct_chol         bmi        age\ndirect_chol   1.0000000 -0.34679048 0.10827307\nbmi          -0.3467905  1.00000000 0.03385215\nage           0.1082731  0.03385215 1.00000000\n\n\nUsing the hypothetical threshold of 0.8, we can conclude that the predictors are not collerated. In the code below, we are going to fit both models using the fit function. After which we are going to collect and combine predictions, and load them.\nIn the code below we are going to specify a recipe object after which we will add steps for engineering our features (feature engineering). The steps are to preprocess the data into a form that will allegedly improve our analysis.\n\nset.seed(123)\nlr_recipe&lt;-recipe(diabetes~.,data = ml_training) |&gt;\n  step_log(all_numeric()) |&gt; \n  step_normalize(all_numeric()) |&gt; #Centering and scaling\n  step_dummy(all_nominal(), -all_outcomes())\n\n\nlr_worflow&lt;-workflow() |&gt; \n  add_model(lr_model) |&gt; \n  add_recipe(lr_recipe)\n\n\nset.seed(123)\nlr_worflow_fit&lt;-lr_worflow |&gt; \n  last_fit(split = ml_split)\n\nlr_worflow_fit |&gt; \n  collect_metrics()\n\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary        0.909  Preprocessor1_Model1\n2 roc_auc     binary        0.790  Preprocessor1_Model1\n3 brier_class binary        0.0743 Preprocessor1_Model1\n\nlr_resultss&lt;-lr_worflow_fit |&gt; \n  collect_predictions()\n\nlr_resultss\n\n# A tibble: 1,358 × 7\n   .pred_class .pred_Yes .pred_No id                .row diabetes .config       \n   &lt;fct&gt;           &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;            &lt;int&gt; &lt;fct&gt;    &lt;chr&gt;         \n 1 No             0.0774    0.923 train/test split     4 No       Preprocessor1…\n 2 No             0.0941    0.906 train/test split    10 No       Preprocessor1…\n 3 No             0.103     0.897 train/test split    11 No       Preprocessor1…\n 4 No             0.101     0.899 train/test split    12 No       Preprocessor1…\n 5 No             0.112     0.888 train/test split    14 No       Preprocessor1…\n 6 No             0.0235    0.976 train/test split    15 No       Preprocessor1…\n 7 No             0.207     0.793 train/test split    19 No       Preprocessor1…\n 8 No             0.131     0.869 train/test split    22 Yes      Preprocessor1…\n 9 No             0.122     0.878 train/test split    24 Yes      Preprocessor1…\n10 No             0.0877    0.912 train/test split    28 No       Preprocessor1…\n# ℹ 1,348 more rows"
  },
  {
    "objectID": "nhanes_ml_project.html#model-metrics",
    "href": "nhanes_ml_project.html#model-metrics",
    "title": "MLearning",
    "section": "Model Metrics",
    "text": "Model Metrics\nIn this section we are going to visualize the model results\n\nset.seed(123)\nlr_resultss |&gt; \n  conf_mat(truth = diabetes,\n           estimate = .pred_class)\n\n          Truth\nPrediction  Yes   No\n       Yes    4    8\n       No   116 1230\n\n\n\nThe logistic regression correctly classifies 1237 out 1358 individuals (91%).\n116 false negatives\n8 false positives\n\nTo check other metrics, we are going to create a metric set\n\nset.seed(112)\nlr_metric&lt;-metric_set(sens,accuracy, yardstick::spec)\n\n\n\nlr_resultss |&gt; \n  lr_metric(truth = diabetes, \n            estimate = .pred_class)\n\n# A tibble: 3 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 sens     binary        0.0333\n2 accuracy binary        0.909 \n3 spec     binary        0.994 \n\n\n\nset.seed(123)\nlr_resultss |&gt;\n  roc_curve(truth = diabetes,.pred_Yes) |&gt;\n  autoplot()\n\n\n\n\n\n\n\nroc_auc(lr_resultss, truth = diabetes, .pred_Yes)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.790\n\n\n\nset.seed(123)\nheatmap_lr&lt;-conf_mat(lr_resultss, truth = diabetes, estimate = .pred_class) |&gt; \n  autoplot(type = \"heatmap\")\n\n\nmosaic_lr&lt;-conf_mat(lr_resultss, truth = diabetes, estimate = .pred_class) |&gt; \n  autoplot(type = \"mosaic\")\n\ncowplot::plot_grid(mosaic_lr,heatmap_lr)\n\n\n\n\n\n\n\n\nThe results from the confusion matrix, metrics and plots show that the model is excellent at predicting people that do not have diabetes, hence it has a low false positive rate. Even though the accuracy of the model is 91.1%, the model struggles to correctly predict people that actually have diabetes, making accuracy not the ideal measure in this case. Out of 120 positive cases, the model only predicts 4 cases. To add more nuance to the results we will also plot the ROC curve, and check the area under the curve which shows the models discriminative ability.\n\nset.seed(123)\nroc_auc(lr_resultss,truth = diabetes,.pred_Yes)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.790\n\nlr_roc_plot&lt;-lr_resultss |&gt; \n  roc_curve(truth = diabetes, .pred_Yes) |&gt; \n  autoplot()\n  \nlr_roc_plot\n\n\n\n\n\n\n\n\n\nRandom Forest\nWe are going to try a different model, Random Forest to predict diabetes, as our previous model could only predict correctly negative cases.\n\nrf&lt;-rand_forest() |&gt; \n  set_args(mtry = 10) |&gt; \n  set_engine(\"ranger\") |&gt; \n  set_mode(\"classification\")\n\n\nset.seed(123)\nrf_recipe&lt;-recipe(diabetes~.,data = ml_training) |&gt; \n  step_log(all_numeric()) |&gt; \n  step_normalize(all_numeric()) |&gt; \n  step_dummy(all_nominal(),-all_outcomes())\n\n\nrf_workflow&lt;-workflow() |&gt; \n  add_model(rf) |&gt; \n  add_recipe(rf_recipe)\n\n\nset.seed(123)\nrf_wrkflw_fit&lt;-rf_workflow |&gt; \n  tune::last_fit(split = ml_split)\n\n→ A | warning: ! 10 columns were requested but there were 9 predictors in the data.\n               ℹ 9 predictors will be used.\n\n\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x1\n\n\n\n\nrf_wrkflw_fit |&gt; \n  collect_metrics()\n\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary        0.935  Preprocessor1_Model1\n2 roc_auc     binary        0.896  Preprocessor1_Model1\n3 brier_class binary        0.0551 Preprocessor1_Model1\n\nrf_results&lt;-rf_wrkflw_fit |&gt; \n  collect_predictions()\n\n\nset.seed(123)\nmosaic_rf&lt;-rf_results |&gt; \n  conf_mat(truth = diabetes,.pred_class) |&gt; \n  autoplot(type = \"mosaic\")\n\nheatmap_rf&lt;-rf_results |&gt; \n  conf_mat(truth = diabetes,.pred_class) |&gt;\n  autoplot(type = \"heatmap\")\n  \ncowplot::plot_grid(heatmap_rf,mosaic_rf)\n\n\n\n\n\n\n\n\n\nset.seed(123)\nrf_metrics&lt;-metric_set(yardstick::sens, yardstick::spec, yardstick::accuracy)\n\n\nlr_resultss |&gt; \n  rf_metrics(truth = diabetes, \n            estimate = .pred_class)\n\n# A tibble: 3 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 sens     binary        0.0333\n2 spec     binary        0.994 \n3 accuracy binary        0.909 \n\nrf_results |&gt; \n  rf_metrics(truth = diabetes, estimate = .pred_class)\n\n# A tibble: 3 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 sens     binary         0.35 \n2 spec     binary         0.992\n3 accuracy binary         0.935\n\nrf_results |&gt; \n  roc_auc(truth = diabetes,.pred_Yes)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.896\n\n\n\nrf_roc_plot&lt;-rf_results |&gt; \n  roc_curve(truth = diabetes,.pred_Yes) |&gt; \n  autoplot()\n\n\n\nModel comparison\n\ncowplot::plot_grid(rf_roc_plot, lr_roc_plot,labels = c(\"Random Forest\", \"Logistic Regression (Classification)\"),label_size = 12)\n\n\n\n\n\n\n\n\nThe random Forest model performs better that logistic regression my almost all metrics.\n\nAccuracy : 0.93\nSensitivity : 0.34\nSpecificity : 0.991\nROC-AUC : 0.886\n\nThe random forest model improves sensitivity from 4.07% (in logistic regression) to 34% (Random Forest), meaning that the model is relatively better at identifying positive cases compared to logistic regression, even as it still fails to predict around 64% of the positive cases correctly.\nNote that other model buiding practices such as hyperparameter tuning (k-fold cross validation) have been skipped.\n\n\nUsing the model (Use case)\n\npatient1&lt;-tribble(~age,~bmi,~direct_chol,~marital_status,~gender,\n                  40,27.3,1.5,\"Divorced\",\"female\")\n\nrf_pred&lt;-rf_wrkflw_fit |&gt; \n  extract_workflow()\n\npredict(rf_pred, new_data = patient1)\n\n# A tibble: 1 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 No         \n\n\nGiven the dataset, we can predict that our patient (patient 1) does not have diabetes. Remember the model has a high accuracy and high specificity (excels at identifying negative cases)."
  }
]