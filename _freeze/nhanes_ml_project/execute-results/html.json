{
  "hash": "d9b34b3280ab84c36b141c9b0492dc99",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"MLearning\"\nformat: html\neditor: visual\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n## Introduction\n\nIn this project we will use the NHANES dataset to predict diabetes given the available risk factors. The National Health ans Nutrition Survey is a a program in the US designed to assess the health and nutritional status pf adults, and children in the US. The data includes demographic, socio-economic, dietary, and health-related information.\n\n### Loading packages including the dataset\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(NHANES)\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n✔ broom        1.0.8     ✔ recipes      1.3.0\n✔ dials        1.4.0     ✔ rsample      1.3.0\n✔ dplyr        1.1.4     ✔ tibble       3.2.1\n✔ ggplot2      3.5.2     ✔ tidyr        1.3.1\n✔ infer        1.0.8     ✔ tune         1.3.0\n✔ modeldata    1.4.0     ✔ workflows    1.2.0\n✔ parsnip      1.3.1     ✔ workflowsets 1.1.0\n✔ purrr        1.0.4     ✔ yardstick    1.3.2\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ lubridate 1.9.4     ✔ stringr   1.5.1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ stringr::fixed()    masks recipes::fixed()\n✖ dplyr::lag()        masks stats::lag()\n✖ readr::spec()       masks yardstick::spec()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(janitor)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(cowplot)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'cowplot'\n\nThe following object is masked from 'package:lubridate':\n\n    stamp\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(gtsummary)\nlibrary(ranger)\nlibrary(randomForest)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nrandomForest 4.7-1.2\nType rfNews() to see new features/changes/bug fixes.\n\nAttaching package: 'randomForest'\n\nThe following object is masked from 'package:ranger':\n\n    importance\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n```\n\n\n:::\n:::\n\n\n\n### Inspecting the dataset\n\nWe are going to save the dataset into the nhanes_df object to maintain the original dataset intact.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnhanes_df<-NHANES |> \n  select(Diabetes,DirectChol,BMI,MaritalStatus,Age,Gender) |>\n  drop_na() |> \n  clean_names()\n\n# Changing the levels for appropriate analysis\nnhanes_df<-nhanes_df |> \n  mutate(diabetes=factor(diabetes, \n                         levels = c(\"Yes\", \"No\"))) |> \n           glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 6,786\nColumns: 6\n$ diabetes       <fct> No, No, No, No, No, No, No, No, No, No, No, No, No, No,…\n$ direct_chol    <dbl> 1.29, 1.29, 1.29, 1.16, 2.12, 2.12, 2.12, 0.67, 0.96, 1…\n$ bmi            <dbl> 32.22, 32.22, 32.22, 30.57, 27.24, 27.24, 27.24, 23.67,…\n$ marital_status <fct> Married, Married, Married, LivePartner, Married, Marrie…\n$ age            <int> 34, 34, 34, 49, 45, 45, 45, 66, 58, 54, 58, 50, 33, 60,…\n$ gender         <fct> male, male, male, female, female, female, female, male,…\n```\n\n\n:::\n:::\n\n\n\nIn the code belwo, we are splitting our data into training and testing sets (0.8, 0.2) and stratify by the target variable so that we do not end up having all the data from the target variable.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123) #For reproducubility\nml_split<-initial_split(nhanes_df,\n                        prop = 0.8,\n                        strata = diabetes)\n\nml_training<-ml_split |> \n  training()\n\nml_test<-ml_split |>\n  testing()\n```\n:::\n\n\n\nWe are going to specify 2 models, logistic regression, and Random Forest.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlr_model<-logistic_reg() |> \n  set_engine(\"glm\") |> \n  set_mode(\"classification\") \n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nml_training |> \n  select_if(is.numeric) |> \n  cor()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            direct_chol         bmi        age\ndirect_chol   1.0000000 -0.34679048 0.10827307\nbmi          -0.3467905  1.00000000 0.03385215\nage           0.1082731  0.03385215 1.00000000\n```\n\n\n:::\n:::\n\n\n\nUsing the hypothetical threshold of 0.8, we can conclude that the predictors are not collerated. In the code below, we are going to fit both models using the fit function. After which we are going to collect and combine predictions, and load them.\n\nIn the code below we are going to specify a recipe object after which we will add steps for engineering our features (feature engineering). The steps are to preprocess the data into a form that will allegedly improve our analysis.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlr_recipe<-recipe(diabetes~.,data = ml_training) |>\n  step_log(all_numeric()) |> \n  step_normalize(all_numeric()) |> #Centering and scaling\n  step_dummy(all_nominal(), -all_outcomes())\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlr_worflow<-workflow() |> \n  add_model(lr_model) |> \n  add_recipe(lr_recipe)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlr_worflow_fit<-lr_worflow |> \n  last_fit(split = ml_split)\n\nlr_worflow_fit |> \n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config             \n  <chr>       <chr>          <dbl> <chr>               \n1 accuracy    binary        0.909  Preprocessor1_Model1\n2 roc_auc     binary        0.790  Preprocessor1_Model1\n3 brier_class binary        0.0743 Preprocessor1_Model1\n```\n\n\n:::\n\n```{.r .cell-code}\nlr_resultss<-lr_worflow_fit |> \n  collect_predictions()\n\nlr_resultss\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1,358 × 7\n   .pred_class .pred_Yes .pred_No id                .row diabetes .config       \n   <fct>           <dbl>    <dbl> <chr>            <int> <fct>    <chr>         \n 1 No             0.0774    0.923 train/test split     4 No       Preprocessor1…\n 2 No             0.0941    0.906 train/test split    10 No       Preprocessor1…\n 3 No             0.103     0.897 train/test split    11 No       Preprocessor1…\n 4 No             0.101     0.899 train/test split    12 No       Preprocessor1…\n 5 No             0.112     0.888 train/test split    14 No       Preprocessor1…\n 6 No             0.0235    0.976 train/test split    15 No       Preprocessor1…\n 7 No             0.207     0.793 train/test split    19 No       Preprocessor1…\n 8 No             0.131     0.869 train/test split    22 Yes      Preprocessor1…\n 9 No             0.122     0.878 train/test split    24 Yes      Preprocessor1…\n10 No             0.0877    0.912 train/test split    28 No       Preprocessor1…\n# ℹ 1,348 more rows\n```\n\n\n:::\n:::\n\n\n\n## Model Metrics\n\nIn this section we are going to visualize the model results\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlr_resultss |> \n  conf_mat(truth = diabetes,\n           estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Truth\nPrediction  Yes   No\n       Yes    4    8\n       No   116 1230\n```\n\n\n:::\n:::\n\n\n\n-   The logistic regression correctly classifies 1237 out 1358 individuals (91%).\n\n-   118 false negatives\n\n-   3 false positives\n\nTo check other metrics, we are going to create a metric set\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlr_metric<-metric_set(sens,accuracy, yardstick::spec)\n\n\n\nlr_resultss |> \n  lr_metric(truth = diabetes, \n            estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 sens     binary        0.0333\n2 accuracy binary        0.909 \n3 spec     binary        0.994 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlr_resultss |>\n  roc_curve(truth = diabetes,.pred_Yes) |>\n  autoplot()\n```\n\n::: {.cell-output-display}\n![](nhanes_ml_project_files/figure-html/Model metrics-1.png){width=672}\n:::\n\n```{.r .cell-code}\nroc_auc(lr_resultss, truth = diabetes, .pred_Yes)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.790\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nheatmap_lr<-conf_mat(lr_resultss, truth = diabetes, estimate = .pred_class) |> \n  autoplot(type = \"heatmap\")\n\n\nmosaic_lr<-conf_mat(lr_resultss, truth = diabetes, estimate = .pred_class) |> \n  autoplot(type = \"mosaic\")\n\ncowplot::plot_grid(mosaic_lr,heatmap_lr)\n```\n\n::: {.cell-output-display}\n![](nhanes_ml_project_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\nThe results from the confusion matrix, metrics and plots show that the model is excellent at predicting people that do not have diabetes, hence it has a low false positive rate. Even though the accuracy of the model is 91.1%, the model struggles to correctly predict people that actually have diabetes, making accuracy not the ideal measure in this case. Out of 123 positive cases, the model only predicts 5 cases. To add more nuance to the results we will also plot the ROC curve, and check the area under the curve which shows the models discriminative ability.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nroc_auc(lr_resultss,truth = diabetes,.pred_Yes)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.790\n```\n\n\n:::\n\n```{.r .cell-code}\nlr_roc_plot<-lr_resultss |> \n  roc_curve(truth = diabetes, .pred_Yes) |> \n  autoplot()\n  \nlr_roc_plot\n```\n\n::: {.cell-output-display}\n![](nhanes_ml_project_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n### Random Forest\n\nWe are going to try a different model, Random Forest to predict diabetes, as our previous model could only predict correctly negative cases.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf<-rand_forest() |> \n  set_args(mtry = 10) |> \n  set_engine(\"ranger\") |> \n  set_mode(\"classification\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_recipe<-recipe(diabetes~.,data = ml_training) |> \n  step_log(all_numeric()) |> \n  step_normalize(all_numeric()) |> \n  step_dummy(all_nominal(),-all_outcomes())\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_workflow<-workflow() |> \n  add_model(rf) |> \n  add_recipe(rf_recipe)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_wrkflw_fit<-rf_workflow |> \n  tune::last_fit(split = ml_split)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n→ A | warning: ! 10 columns were requested but there were 9 predictors in the data.\n               ℹ 9 predictors will be used.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n```{.r .cell-code}\nrf_wrkflw_fit |> \n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config             \n  <chr>       <chr>          <dbl> <chr>               \n1 accuracy    binary        0.935  Preprocessor1_Model1\n2 roc_auc     binary        0.895  Preprocessor1_Model1\n3 brier_class binary        0.0547 Preprocessor1_Model1\n```\n\n\n:::\n\n```{.r .cell-code}\nrf_results<-rf_wrkflw_fit |> \n  collect_predictions()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmosaic_rf<-rf_results |> \n  conf_mat(truth = diabetes,.pred_class) |> \n  autoplot(type = \"mosaic\")\n\nheatmap_rf<-rf_results |> \n  conf_mat(truth = diabetes,.pred_class) |>\n  autoplot(type = \"heatmap\")\n  \ncowplot::plot_grid(heatmap_rf,mosaic_rf)\n```\n\n::: {.cell-output-display}\n![](nhanes_ml_project_files/figure-html/Conf Matrix-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_metrics<-metric_set(yardstick::sens, yardstick::spec, yardstick::accuracy)\n\nrf_results |> \n  rf_metrics(truth = diabetes, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 sens     binary         0.35 \n2 spec     binary         0.992\n3 accuracy binary         0.935\n```\n\n\n:::\n\n```{.r .cell-code}\nrf_results |> \n  roc_auc(truth = diabetes,.pred_Yes)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.895\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_roc_plot<-rf_results |> \n  roc_curve(truth = diabetes,.pred_Yes) |> \n  autoplot()\n```\n:::\n\n\n\n### Model comparison\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncowplot::plot_grid(rf_roc_plot, lr_roc_plot)\n```\n\n::: {.cell-output-display}\n![](nhanes_ml_project_files/figure-html/Model comparison-1.png){width=672}\n:::\n:::\n\n\n\nThe random Forest model performs better that logistic regression my almost all metrics.\n\n-   Accuracy : 0.934\n\n-   Sensitivity : 0.358\n\n-   Specificity : 0.991\n\n-   ROC-AUC : 0.886\n\nThe random forest model improves sensitivity from 4.07% (in logistic regression) to 35.8%, meaning that the model is relatively better at identifying positive cases compared to logistic regression, even as it still fails to predict around 64% of the positive cases correctly.\n\nNote that other model buiding practices such as hyperparameter tuning (k-fold cross validation) have been skipped.\n\n### Using the model\\>\\>\\>\\>\\>\\>\\>\\>\\>\\>\\>\\>Loading\n",
    "supporting": [
      "nhanes_ml_project_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}